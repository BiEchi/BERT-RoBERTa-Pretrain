# roberta_pretrain
HuggingFace reproduction of the RoBERTa pretraining pipeline, with memory optimization using DeepSpeed.


wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/text-classification/run_glue.py

wget https://raw.githubusercontent.com/huggingface/transformers/main/examples/pytorch/language-modeling/run_mlm.py
